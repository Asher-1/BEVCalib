"""
Custom Dataset Loader for BEVCalib
ä¸“é—¨ç”¨äºåŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆå¦‚ B26Aï¼‰ï¼Œå¤„ç†éæ ‡å‡† KITTI æ ¼å¼çš„æ•°æ®

ä¸»è¦ç‰¹æ€§:
1. è‡ªåŠ¨æ£€æµ‹ç‚¹äº‘åæ ‡ç³»ï¼ˆLiDAR vs Cameraï¼‰
2. è‡ªåŠ¨è½¬æ¢åˆ° BEV åæ ‡ç³»
3. æ”¯æŒæ›´å¤§çš„æ¢æµ‹èŒƒå›´ï¼ˆå¯é…ç½®ï¼‰
4. å…¼å®¹ KITTI-Odometry ç›®å½•ç»“æ„
"""

from pykitti import odometry
import numpy as np
import open3d as o3d
from PIL import Image
from torch.utils.data import Dataset
import os
from tqdm import tqdm

# ä» bev_settings å¯¼å…¥ä½“ç´ åŒ–èŒƒå›´é…ç½®
from bev_settings import xbound, ybound, zbound


class CustomDataset(Dataset):
    """
    KITTI-Odometry æ ¼å¼æ•°æ®é›†åŠ è½½å™¨
    
    æ”¯æŒï¼š
    1. æ ‡å‡† KITTI-Odometry æ•°æ®é›†ï¼ˆ22ä¸ªåºåˆ—ï¼‰
    2. è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆè‡ªåŠ¨æ£€æµ‹åºåˆ—ï¼‰
    
    åæ ‡ç³»è¯´æ˜ï¼š
    - è¾“å…¥ç‚¹äº‘åæ ‡ç³»ï¼šSensingç³»ï¼ˆXå‰è¿›ï¼ŒYå·¦ï¼ŒZä¸Šï¼‰
    - TrçŸ©é˜µï¼šSensing â†’ Camera
    - è¾“å‡ºç‚¹äº‘åæ ‡ç³»ï¼šBEVåæ ‡ç³»ï¼ˆXå‰è¿›ï¼ŒYå·¦ï¼ŒZä¸Šï¼‰ï¼ŒèŒƒå›´è£å‰ªåˆ°ä½“ç´ åŒ–èŒƒå›´
    - è¿”å›çš„ gt_transform æ˜¯ Tr çš„é€†çŸ©é˜µï¼ˆCamera â†’ Sensingï¼‰
    
    æ•°æ®åˆ©ç”¨ç‡ç»Ÿè®¡ï¼š
    - è®°å½•åŸå§‹ç‚¹äº‘æ•°é‡ã€è¿‡æ»¤åç‚¹äº‘æ•°é‡ã€åˆ©ç”¨ç‡
    - æ”¯æŒè®­ç»ƒå‰éªŒè¯æ•°æ®åˆ©ç”¨ç‡
    """
    
    # æ ‡å‡†KITTI-Odometryåºåˆ—
    KITTI_SEQUENCES = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', 
                       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21']
    
    def __init__(self, data_folder='./data/kitti-odemetry', suf='.png', sequences=None, auto_detect=True):
        # ä½¿ç”¨ bev_settings çš„ä½“ç´ åŒ–èŒƒå›´é…ç½®
        self.x_min, self.x_max = xbound[0], xbound[1]
        self.y_min, self.y_max = ybound[0], ybound[1]
        self.z_min, self.z_max = zbound[0], zbound[1]
        
        # æ•°æ®åˆ©ç”¨ç‡ç»Ÿè®¡
        self.utilization_stats = {
            'total_original_points': 0,
            'total_filtered_points': 0,
            'total_ego_filtered_points': 0,
            'total_range_filtered_points': 0,
            'low_point_frames': 0,
            'null_frames': 0,
            'valid_frames': 0,
        }
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_folder: æ•°æ®é›†æ ¹ç›®å½•
            suf: å›¾åƒæ–‡ä»¶åç¼€
            sequences: æŒ‡å®šåºåˆ—åˆ—è¡¨ï¼Œå¦‚ ['00', '01']ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨ auto_detect
            auto_detect: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹å¯ç”¨åºåˆ—ï¼ˆé»˜è®¤ Trueï¼‰
                - True: è‡ªåŠ¨æ‰«æ sequences/ ç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰æ•ˆåºåˆ—
                - False: ä½¿ç”¨æ ‡å‡† KITTI åºåˆ—åˆ—è¡¨
        """
        self.all_files = []
        self.dataset_root = data_folder
        self.K = {}
        self.T = {}
        
        # ç¡®å®šè¦ä½¿ç”¨çš„åºåˆ—
        if sequences is not None:
            self.sequences = sequences
        elif auto_detect:
            self.sequences = self._detect_sequences()
            if not self.sequences:
                # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†KITTIåºåˆ—
                print(f"[CustomDataset] è‡ªåŠ¨æ£€æµ‹æœªæ‰¾åˆ°åºåˆ—ï¼Œå°è¯•æ ‡å‡†KITTIåºåˆ—")
                self.sequences = self.KITTI_SEQUENCES
        else:
            self.sequences = self.KITTI_SEQUENCES
        
        print(f"[CustomDataset] ä½¿ç”¨åºåˆ—: {self.sequences}")
        
        loaded_sequences = []
        for seq in self.sequences:
            try:
                odom = odometry(data_folder, seq)
                calib = odom.calib
                # æ³¨æ„: pykittiçš„T_cam2_veloå®é™…ä¸Šæ˜¯ velo->cam çš„å˜æ¢çŸ©é˜µ
                # å˜æ¢ç‚¹äº‘åˆ°ç›¸æœºåæ ‡ç³»: P_cam = T_cam2_velo @ P_velo
                # ä¸éœ€è¦å–é€†ï¼
                T_velo_to_cam = calib.T_cam2_velo
                self.K[seq] = calib.K_cam2
                self.T[seq] = T_velo_to_cam
                
                image_list = os.listdir(os.path.join(self.dataset_root, 'sequences', seq, 'image_2'))
                image_list.sort()

                frame_count = 0
                for image_name in image_list:
                    base_name = image_name.split('.')[0]
                    if not os.path.exists(os.path.join(self.dataset_root, 'sequences', seq, 'velodyne',
                                                       base_name + '.bin')):
                        continue
                    if not os.path.exists(os.path.join(self.dataset_root, 'sequences', seq, 'image_2',
                                                       base_name + suf)):
                        continue

                    self.all_files.append(os.path.join(seq, base_name))
                    frame_count += 1
                
                if frame_count > 0:
                    loaded_sequences.append(seq)
                    print(f"  âœ“ åºåˆ— {seq}: {frame_count} å¸§")
            except Exception as e:
                # åºåˆ—ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œè·³è¿‡
                continue
        
        if not self.all_files:
            raise ValueError(f"æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼æ£€æŸ¥è·¯å¾„: {data_folder}")
        
        print(f"[CustomDataset] æ€»è®¡: {len(self.all_files)} å¸§æ¥è‡ª {len(loaded_sequences)} ä¸ªåºåˆ—")
    
    def _detect_sequences(self):
        """è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ä¸­å­˜åœ¨çš„åºåˆ—"""
        sequences = []
        sequences_dir = os.path.join(self.dataset_root, 'sequences')
        
        if not os.path.exists(sequences_dir):
            return sequences
        
        for item in os.listdir(sequences_dir):
            seq_path = os.path.join(sequences_dir, item)
            
            if not os.path.isdir(seq_path):
                continue
            
            # æ£€æŸ¥å¿…è¦çš„å­ç›®å½•å’Œæ–‡ä»¶
            image_dir = os.path.join(seq_path, 'image_2')
            velodyne_dir = os.path.join(seq_path, 'velodyne')
            calib_file = os.path.join(seq_path, 'calib.txt')
            
            if os.path.exists(image_dir) and os.path.exists(velodyne_dir) and os.path.exists(calib_file):
                sequences.append(item)
        
        sequences.sort()
        return sequences

    def __len__(self):
        return len(self.all_files)
    
    def __getitem__(self, idx, track_stats=False):
        seq = self.all_files[idx].split('/')[0]
        id = self.all_files[idx].split('/')[1]
        img_path = os.path.join(self.dataset_root, 'sequences', seq, 'image_2', id+'.png')
        pcd_path = os.path.join(self.dataset_root, 'sequences', seq, 'velodyne', id+'.bin')
        if not os.path.exists(img_path) or not os.path.exists(pcd_path):
            print('File not exist')
            assert False
        img = Image.open(img_path)
        img.resize((1242, 375))
        pcd = np.fromfile(pcd_path, dtype=np.float32).reshape(-1, 4)
        
        # è®°å½•åŸå§‹ç‚¹äº‘æ•°é‡
        original_points = len(pcd)
        
        # ========== åæ ‡ç³»è½¬æ¢ ==========
        # ä½¿ç”¨ bev_settings.py ä¸­çš„ä½“ç´ åŒ–èŒƒå›´é…ç½®
        # =============================================
        
        # æ­¥éª¤1: è¿‡æ»¤è‡ªè½¦å‘¨å›´çš„ç‚¹ï¼ˆé¿å…è‡ªè½¦ç‚¹äº‘å¹²æ‰°ï¼‰
        ego_filter = (np.abs(pcd[:, 0]) > 3.) | (np.abs(pcd[:, 1]) > 3.)
        after_ego_filter = np.sum(ego_filter)
        pcd = pcd[ego_filter, :]
        
        # æ­¥éª¤2: è£å‰ªåˆ°ä½“ç´ åŒ–èŒƒå›´ï¼ˆä½¿ç”¨ bev_settings é…ç½®ï¼‰
        range_filter = (pcd[:, 0] >= self.x_min) & (pcd[:, 0] <= self.x_max) & \
                       (pcd[:, 1] >= self.y_min) & (pcd[:, 1] <= self.y_max) & \
                       (pcd[:, 2] >= self.z_min) & (pcd[:, 2] <= self.z_max)
        pcd = pcd[range_filter, :]
        
        final_points = len(pcd)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        if track_stats:
            self.utilization_stats['total_original_points'] += original_points
            self.utilization_stats['total_ego_filtered_points'] += after_ego_filter
            self.utilization_stats['total_filtered_points'] += final_points
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‚¹
        if len(pcd) < 100:
            if track_stats:
                self.utilization_stats['low_point_frames'] += 1
            # å¦‚æœç‚¹å¤ªå°‘ï¼Œæ‰©å¤§èŒƒå›´é‡æ–°è¿‡æ»¤
            pcd = np.fromfile(pcd_path, dtype=np.float32).reshape(-1, 4)
            ego_filter = (np.abs(pcd[:, 0]) > 2.) | (np.abs(pcd[:, 1]) > 2.)
            pcd = pcd[ego_filter, :]
            # æ”¾å®½ Z è½´é™åˆ¶
            range_filter = (pcd[:, 0] >= self.x_min) & (pcd[:, 0] <= self.x_max) & \
                           (pcd[:, 1] >= self.y_min) & (pcd[:, 1] <= self.y_max)
            pcd = pcd[range_filter, :]
            
            # å¦‚æœä»ç„¶å¤ªå°‘ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç‚¹äº‘ï¼ˆé¿å…è®­ç»ƒå´©æºƒï¼‰
            if len(pcd) < 10:
                if track_stats:
                    self.utilization_stats['null_frames'] += 1
                print(f"âš ï¸ å¸§ {seq}/{id} æ˜¯å¼‚å¸¸å¸§ï¼ˆå»ç•¸å˜å¯èƒ½å¤±è´¥ï¼‰ï¼Œä½¿ç”¨è™šæ‹Ÿç‚¹äº‘")
                # åˆ›å»ºä¸€ä¸ªå°çš„è™šæ‹Ÿç‚¹äº‘ï¼Œä½äºåŸç‚¹é™„è¿‘
                x_center = (self.x_min + self.x_max) / 2
                pcd = np.array([
                    [x_center + 10.0, 0.0, 0.0, 0.0],
                    [x_center + 20.0, 0.0, 0.0, 0.0],
                    [x_center + 30.0, 0.0, 0.0, 0.0],
                    [x_center + 40.0, 0.0, 0.0, 0.0],
                    [x_center + 50.0, 0.0, 0.0, 0.0],
                ], dtype=np.float32)
        
        if track_stats:
            self.utilization_stats['valid_frames'] += 1
        
        gt_transform = self.T[seq]
        intrinsic = self.K[seq]
        return img, pcd, gt_transform, intrinsic
    
    def validate_data_utilization(self, sample_ratio=0.1, min_utilization=0.3, min_valid_ratio=0.9, verbose=True):
        """
        éªŒè¯æ•°æ®åˆ©ç”¨ç‡
        
        Args:
            sample_ratio: é‡‡æ ·æ¯”ä¾‹ï¼Œç”¨äºå¿«é€ŸéªŒè¯ (0.0-1.0)
            min_utilization: æœ€ä½ç‚¹äº‘åˆ©ç”¨ç‡é˜ˆå€¼
            min_valid_ratio: æœ€ä½æœ‰æ•ˆå¸§æ¯”ä¾‹é˜ˆå€¼
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            dict: éªŒè¯ç»“æœï¼ŒåŒ…å« 'passed' å¸ƒå°”å€¼å’Œè¯¦ç»†ç»Ÿè®¡
        """
        # é‡ç½®ç»Ÿè®¡
        self.utilization_stats = {
            'total_original_points': 0,
            'total_filtered_points': 0,
            'total_ego_filtered_points': 0,
            'total_range_filtered_points': 0,
            'low_point_frames': 0,
            'null_frames': 0,
            'valid_frames': 0,
        }
        
        # é‡‡æ ·éªŒè¯
        num_samples = max(1, int(len(self) * sample_ratio))
        sample_indices = np.random.choice(len(self), num_samples, replace=False)
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"æ•°æ®åˆ©ç”¨ç‡éªŒè¯ (é‡‡æ · {num_samples}/{len(self)} å¸§, {sample_ratio*100:.1f}%)")
            print(f"BEVèŒƒå›´é…ç½®: X=[{self.x_min}, {self.x_max}], Y=[{self.y_min}, {self.y_max}], Z=[{self.z_min}, {self.z_max}]")
            print(f"{'='*60}")
        
        for idx in tqdm(sample_indices, desc="éªŒè¯æ•°æ®åˆ©ç”¨ç‡", disable=not verbose):
            try:
                self.__getitem__(idx, track_stats=True)
            except Exception as e:
                self.utilization_stats['null_frames'] += 1
                if verbose:
                    print(f"  âš ï¸ å¸§ {idx} åŠ è½½å¤±è´¥: {e}")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        stats = self.utilization_stats
        total_frames = num_samples
        valid_frames = stats['valid_frames']
        null_frames = stats['null_frames']
        low_point_frames = stats['low_point_frames']
        
        # ç‚¹äº‘åˆ©ç”¨ç‡ = è¿‡æ»¤åç‚¹æ•° / åŸå§‹ç‚¹æ•°
        if stats['total_original_points'] > 0:
            point_utilization = stats['total_filtered_points'] / stats['total_original_points']
            ego_filter_ratio = stats['total_ego_filtered_points'] / stats['total_original_points']
        else:
            point_utilization = 0
            ego_filter_ratio = 0
        
        # æœ‰æ•ˆå¸§æ¯”ä¾‹
        valid_ratio = valid_frames / total_frames if total_frames > 0 else 0
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        passed = point_utilization >= min_utilization and valid_ratio >= min_valid_ratio
        
        result = {
            'passed': passed,
            'point_utilization': point_utilization,
            'ego_filter_ratio': ego_filter_ratio,
            'valid_ratio': valid_ratio,
            'total_frames': total_frames,
            'valid_frames': valid_frames,
            'null_frames': null_frames,
            'low_point_frames': low_point_frames,
            'total_original_points': stats['total_original_points'],
            'total_filtered_points': stats['total_filtered_points'],
            'min_utilization': min_utilization,
            'min_valid_ratio': min_valid_ratio,
        }
        
        if verbose:
            print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
            print(f"  - ç‚¹äº‘åˆ©ç”¨ç‡: {point_utilization*100:.2f}% (é˜ˆå€¼: {min_utilization*100:.1f}%)")
            print(f"  - è‡ªè½¦è¿‡æ»¤åä¿ç•™: {ego_filter_ratio*100:.2f}%")
            print(f"  - æœ‰æ•ˆå¸§æ¯”ä¾‹: {valid_ratio*100:.2f}% (é˜ˆå€¼: {min_valid_ratio*100:.1f}%)")
            print(f"  - æ€»å¸§æ•°: {total_frames}, æœ‰æ•ˆå¸§: {valid_frames}, æ— æ•ˆå¸§: {null_frames}, ä½ç‚¹æ•°å¸§: {low_point_frames}")
            print(f"  - åŸå§‹ç‚¹æ•°: {stats['total_original_points']:,}, è¿‡æ»¤å: {stats['total_filtered_points']:,}")
            
            if passed:
                print(f"\nâœ… æ•°æ®åˆ©ç”¨ç‡éªŒè¯é€šè¿‡ï¼")
            else:
                print(f"\nâŒ æ•°æ®åˆ©ç”¨ç‡éªŒè¯æœªé€šè¿‡ï¼")
                if point_utilization < min_utilization:
                    print(f"   - ç‚¹äº‘åˆ©ç”¨ç‡è¿‡ä½ ({point_utilization*100:.2f}% < {min_utilization*100:.1f}%)")
                    print(f"   - å»ºè®®: æ£€æŸ¥ bev_settings.py ä¸­çš„ä½“ç´ åŒ–èŒƒå›´æ˜¯å¦ä¸æ•°æ®é›†åŒ¹é…")
                if valid_ratio < min_valid_ratio:
                    print(f"   - æœ‰æ•ˆå¸§æ¯”ä¾‹è¿‡ä½ ({valid_ratio*100:.2f}% < {min_valid_ratio*100:.1f}%)")
                    print(f"   - å»ºè®®: æ£€æŸ¥æ•°æ®é›†è´¨é‡æˆ–è°ƒæ•´è¿‡æ»¤å‚æ•°")
            print(f"{'='*60}\n")
        
        return result


if __name__ == "__main__":
    # æµ‹è¯•
    dataset_root = '/home/ludahai/develop/data/eol/B26A_online/YR-B26A1-1_20251117_031232_lidar/bevcalib_training_data'
    
    print("=" * 60)
    print("æµ‹è¯• CustomDataset")
    print("=" * 60)
    
    dataset = CustomDataset(
        data_folder=dataset_root,
    )
    
    print(f"\næ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªæ ·æœ¬
    print("\næµ‹è¯•åŠ è½½æ ·æœ¬...")
    img, pcd, gt_transform, intrinsic = dataset[0]
    
    print(f"å›¾åƒå°ºå¯¸: {img.size}")
    print(f"ç‚¹äº‘å½¢çŠ¶: {pcd.shape}")
    print(f"ç‚¹äº‘èŒƒå›´:")
    print(f"  X: [{pcd[:, 0].min():.2f}, {pcd[:, 0].max():.2f}]")
    print(f"  Y: [{pcd[:, 1].min():.2f}, {pcd[:, 1].max():.2f}]")
    print(f"  Z: [{pcd[:, 2].min():.2f}, {pcd[:, 2].max():.2f}]")
    print(f"å˜æ¢çŸ©é˜µå½¢çŠ¶: {gt_transform.shape}")
    print(f"å†…å‚çŸ©é˜µå½¢çŠ¶: {intrinsic.shape}")
    
    print("\nâœ… CustomDataset æµ‹è¯•é€šè¿‡ï¼")

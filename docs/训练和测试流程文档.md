# BEVCalib 训练和测试流程文档

## 1. 环境准备

### 1.1 系统要求

- **操作系统**: Linux (推荐 Ubuntu 18.04+)
- **Python**: 3.11
- **CUDA**: 11.8
- **GPU**: 支持CUDA的NVIDIA GPU（推荐至少8GB显存）

### 1.2 依赖安装

#### 步骤1：创建Conda环境

```bash
conda env create -n bevcalib python=3.11
conda activate bevcalib
```

#### 步骤2：安装CUDA工具包

```bash
conda install -c "nvidia/label/cuda-11.8.0" cuda-toolkit
```

#### 步骤3：安装PyTorch

```bash
pip install torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cu118
```

#### 步骤4：安装其他依赖

```bash
pip install -r requirements.txt
```

主要依赖包括：
- `spconv==2.3.8` (稀疏卷积库)
- `deformable_attention==0.0.20` (可变形注意力)
- `transformers==4.29.1` (Swin Transformer)
- `opencv-python==4.7.0.68`
- `open3d==0.18.0`
- `pykitti==0.3.1`
- `tensorboard` (用于可视化)
- `wandb` (可选，用于实验跟踪)

#### 步骤5：编译CUDA扩展

```bash
cd ./kitti-bev-calib/img_branch/bev_pool
python setup.py build_ext --inplace
```

这一步会编译BEV池化操作的CUDA扩展，是必需的。

### 1.3 Docker方式（可选）

如果使用Docker：

```bash
# 构建镜像
docker build -f Dockerfile/Dockerfile -t bevcalib .

# 运行容器
docker run --gpus all -it -v$(pwd):/workspace bevcalib

# 在容器内编译CUDA扩展
cd ./kitti-bev-calib/img_branch/bev_pool
python setup.py build_ext --inplace
```

## 2. 数据准备

### 2.1 KITTI数据集下载

1. 访问 [KITTI Odometry数据集页面](https://www.cvlibs.net/datasets/kitti/eval_odometry.php)
2. 下载以下文件：
   - **calibration files** (calib.zip)
   - **camera images** (data_odometry_color.zip)
   - **velodyne laser data** (data_odometry_velodyne.zip)
   - **ground truth poses** (data_odometry_poses.zip)

### 2.2 数据集目录结构

下载并解压后，目录结构应为：

```
kitti-odometry/
├── sequences/
│   ├── 00/
│   │   ├── image_2/          # 左相机图像
│   │   │   ├── 000000.png
│   │   │   ├── 000001.png
│   │   │   └── ...
│   │   ├── image_3/          # 右相机图像（可选）
│   │   ├── velodyne/         # 点云数据
│   │   │   ├── 000000.bin
│   │   │   ├── 000001.bin
│   │   │   └── ...
│   │   └── calib.txt         # 标定文件
│   ├── 01/
│   │   └── ...
│   └── ...
└── poses/
    ├── 00.txt                # 序列00的位姿真值
    ├── 01.txt
    └── ...
```

### 2.3 数据集验证

运行以下代码验证数据集是否正确加载：

```python
from kitti_dataset import KittiDataset

dataset = KittiDataset(data_folder='./kitti-odometry')
print(f"Total samples: {len(dataset)}")

# 测试加载一个样本
img, pcd, gt_transform, intrinsic = dataset[0]
print(f"Image shape: {img.size}")
print(f"Point cloud shape: {pcd.shape}")
print(f"GT transform shape: {gt_transform.shape}")
print(f"Intrinsic shape: {intrinsic.shape}")
```

## 3. 模型训练

### 3.1 训练命令

基本训练命令：

```bash
python kitti-bev-calib/train_kitti.py \
    --log_dir ./logs/kitti \
    --dataset_root YOUR_PATH_TO_KITTI/kitti-odometry \
    --save_ckpt_per_epoches 40 \
    --num_epochs 500 \
    --label 20_1.5 \
    --angle_range_deg 20 \
    --trans_range 1.5 \
    --deformable 0 \
    --bev_encoder 1 \
    --batch_size 16 \
    --xyz_only 1 \
    --scheduler 1 \
    --lr 1e-4 \
    --step_size 80
```

### 3.2 训练参数说明

#### 必需参数

- `--dataset_root`: KITTI数据集根目录路径
- `--log_dir`: 日志和模型保存目录

#### 训练配置参数

- `--num_epochs`: 训练轮数（默认：1，建议：500）
- `--batch_size`: 批次大小（默认：4，建议：16，根据GPU显存调整）
- `--lr`: 学习率（默认：5e-5，建议：1e-4）
- `--wd`: 权重衰减（默认：1e-4）

#### 扰动参数

- `--angle_range_deg`: 旋转扰动范围（度）（默认：None，建议：20）
- `--trans_range`: 平移扰动范围（米）（默认：None，建议：1.5）
- `--eval_angle_range_deg`: 验证时的旋转扰动范围（默认：与训练相同）
- `--eval_trans_range`: 验证时的平移扰动范围（默认：与训练相同）

#### 模型配置参数

- `--deformable`: 是否使用可变形注意力（0=否，1=是，默认：-1）
- `--bev_encoder`: 是否使用BEV编码器（0=否，1=是，默认：1）
- `--xyz_only`: 是否只使用点云的XYZ坐标（0=否，1=是，默认：1）

#### 学习率调度参数

- `--scheduler`: 是否使用学习率调度器（0=否，1=是，默认：-1）
- `--step_size`: 学习率衰减步长（默认：100，建议：80）

#### 其他参数

- `--save_ckpt_per_epoches`: 每N个epoch保存一次检查点（默认：-1，表示不保存中间检查点）
- `--eval_epoches`: 每N个epoch进行一次验证（默认：4）
- `--label`: 实验标签，用于区分不同实验（默认：None）
- `--pretrain_ckpt`: 预训练模型路径（用于微调，默认：None）

### 3.3 训练流程详解

#### 3.3.1 数据加载阶段

1. **数据集初始化**：
   ```python
   dataset = KittiDataset(dataset_root)
   ```
   - 扫描所有序列（00-21）
   - 提取每个序列的标定参数（相机内参、LiDAR到相机的变换矩阵）
   - 收集所有有效的图像-点云对

2. **数据集划分**：
   ```python
   train_size = int(0.8 * len(dataset))
   val_size = len(dataset) - train_size
   train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
   ```
   - 80%用于训练，20%用于验证
   - 使用固定随机种子（114514）确保可复现

3. **数据加载器创建**：
   ```python
   train_loader = DataLoader(
       train_dataset,
       batch_size=args.batch_size,
       num_workers=4,
       collate_fn=collate_fn,
       shuffle=True,
       drop_last=True
   )
   ```

#### 3.3.2 数据预处理（collate_fn）

对每个批次的数据进行预处理：

1. **图像处理**：
   ```python
   def crop_and_resize(item, size, intrinsics, crop=True):
       # 将图像裁剪和缩放到目标尺寸 (704, 256)
       # 同时更新相机内参
   ```

2. **点云处理**：
   ```python
   # 填充点云到相同长度
   max_num_points = max([item[1].shape[0] for item in batch])
   for item in batch:
       if pc.shape[0] < max_num_points:
           pc = np.concatenate([pc, padding], axis=0)
   ```

3. **掩码生成**：
   ```python
   masks.append([1] * valid_points + [0] * padding_points)
   ```

#### 3.3.3 训练循环

```python
for epoch in range(num_epochs):
    model.train()
    
    for batch_index, (imgs, pcs, masks, gt_T_to_camera, intrinsics) in enumerate(train_loader):
        # 1. 生成扰动后的初始标定矩阵
        init_T_to_camera, _, _ = generate_single_perturbation_from_T(
            gt_T_to_camera, 
            angle_range_deg=train_noise["angle_range_deg"],
            trans_range=train_noise["trans_range"]
        )
        
        # 2. 数据转换到GPU
        resize_imgs = torch.from_numpy(...).to(device)
        pcs = torch.from_numpy(...).to(device)
        gt_T_to_camera = torch.from_numpy(...).to(device)
        init_T_to_camera = torch.from_numpy(...).to(device)
        
        # 3. 前向传播
        optimizer.zero_grad()
        T_pred, init_loss, loss = model(
            resize_imgs, pcs, gt_T_to_camera, 
            init_T_to_camera, post_cam2ego_T, 
            intrinsic_matrix, masks=masks
        )
        
        # 4. 计算损失
        total_loss = loss["total_loss"]
        
        # 5. 反向传播
        total_loss.backward()
        optimizer.step()
        
        # 6. 记录损失
        if batch_index % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Step [{batch_index+1}], Loss: {total_loss.item():.4f}")
    
    # 7. 学习率调度
    if scheduler_choice:
        scheduler.step()
    
    # 8. 验证
    if epoch % args.eval_epoches == 0:
        validate(model, val_loader, ...)
    
    # 9. 保存检查点
    if (epoch + 1) % args.save_ckpt_per_epoches == 0:
        save_checkpoint(...)
```

#### 3.3.4 扰动生成

在训练时，对真实的标定矩阵添加随机扰动：

```python
def generate_single_perturbation_from_T(T, angle_range_deg, trans_range):
    for i in range(B):
        # 1. 提取原始旋转和平移
        orig_rot = R.from_matrix(T[i, :3, :3])
        orig_trans = T[i, :3, 3]
        
        # 2. 生成随机旋转扰动
        rand_axis = np.random.randn(3)
        rand_axis /= np.linalg.norm(rand_axis)
        rand_angle = np.deg2rad(np.random.uniform(-angle_range_deg, angle_range_deg))
        delta_rot = R.from_rotvec(rand_angle * rand_axis)
        new_rot = delta_rot * orig_rot
        
        # 3. 生成随机平移扰动
        rand_direction = np.random.randn(3)
        rand_direction /= np.linalg.norm(rand_direction)
        delta_trans_magnitude = np.random.uniform(0, trans_range)
        delta_trans = rand_direction * delta_trans_magnitude
        new_trans = orig_trans + delta_trans
        
        # 4. 构建新的变换矩阵
        T_new[i] = construct_T(new_rot, new_trans)
    
    return T_new
```

#### 3.3.5 损失计算

模型输出预测的标定参数后，计算多项损失：

1. **平移损失**：预测平移与真实平移的Smooth L1距离
2. **旋转损失**：预测旋转与真实旋转的四元数距离
3. **四元数归一化损失**：确保预测的四元数是单位四元数
4. **重投影损失**：点云使用预测标定矩阵变换后的3D一致性损失

总损失 = 1.0 × 平移损失 + 0.5 × 旋转损失 + 0.5 × 重投影损失 + 0.5 × 四元数归一化损失

#### 3.3.6 验证阶段

每N个epoch进行一次验证：

```python
if epoch % args.eval_epoches == 0:
    model.eval()
    val_loss = {}
    
    with torch.no_grad():
        for batch in val_loader:
            # 生成验证扰动（可能使用不同的扰动范围）
            init_T = generate_single_perturbation_from_T(
                gt_T, 
                angle_range_deg=eval_angle_range,
                trans_range=eval_trans_range
            )
            
            # 前向传播
            T_pred, _, loss = model(...)
            
            # 累计损失
            for key in loss.keys():
                val_loss[key] += loss[key].item()
    
    # 计算平均损失
    for key in val_loss.keys():
        val_loss[key] /= len(val_loader)
        writer.add_scalar(f"Loss/val/{key}", val_loss[key], epoch)
```

#### 3.3.7 检查点保存

```python
if (epoch + 1) % args.save_ckpt_per_epoches == 0:
    ckpt_path = os.path.join(ckpt_save_dir, f"ckpt_{epoch+1}.pth")
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'train_noise': train_noise,
        'eval_noise': eval_noise,
        'args': vars(args)
    }, ckpt_path)
```

### 3.4 训练监控

#### 3.4.1 TensorBoard

训练过程中会自动记录到TensorBoard：

```bash
tensorboard --logdir ./logs/kitti
```

在浏览器中打开 `http://localhost:6006` 查看：
- 训练损失曲线
- 验证损失曲线
- 各项损失分量

#### 3.4.2 控制台输出

训练过程中会输出：
- 每个epoch的平均损失
- 每10个batch的当前损失
- 验证时的损失和误差

### 3.5 训练技巧

#### 3.5.1 学习率调整

- 初始学习率：1e-4
- 使用StepLR调度器，每80个epoch衰减0.5倍
- 如果损失不下降，可以尝试降低学习率

#### 3.5.2 批次大小调整

- 默认批次大小：16
- 如果GPU显存不足，可以减小到8或4
- 如果显存充足，可以增大到32以加快训练

#### 3.5.3 扰动范围调整

- 默认：角度±20°，平移1.5m
- 如果初始标定误差较大，可以增大扰动范围
- 如果初始标定较准确，可以减小扰动范围以提高精度

#### 3.5.4 模型配置选择

- **deformable=0, bev_encoder=1**：推荐配置，平衡性能和精度
- **deformable=1, bev_encoder=1**：更高精度，但训练更慢
- **deformable=0, bev_encoder=0**：更快训练，但精度可能略低

## 4. 模型测试/推理

### 4.1 下载预训练模型

#### 方式1：Google Drive

```bash
# 安装gdown
pip install gdown

# 下载模型
gdown https://drive.google.com/uc?id=1gWO-Z4NXG2uWwsZPecjWByaZVtgJ0XNb

# 移动到ckpt目录
mv kitti.pth ./ckpt/
```

#### 方式2：Hugging Face

```bash
# 安装huggingface-cli
pip install -U "huggingface_hub[cli]"

# 下载模型
huggingface-cli download cisl-hf/BEVCalib \
    --revision kitti-bev-calib \
    --local-dir ./ckpt
```

### 4.2 推理命令

基本推理命令：

```bash
python kitti-bev-calib/inference_kitti.py \
    --log_dir ./logs/inference \
    --dataset_root YOUR_PATH_TO_KITTI/kitti-odometry \
    --ckpt_path ./ckpt/kitti.pth \
    --angle_range_deg 20.0 \
    --trans_range 1.5 \
    --batch_size 1 \
    --xyz_only 1
```

### 4.3 推理参数说明

- `--dataset_root`: KITTI数据集根目录
- `--ckpt_path`: 模型检查点路径（必需）
- `--log_dir`: 推理结果保存目录
- `--angle_range_deg`: 测试时的旋转扰动范围（默认：20.0）
- `--trans_range`: 测试时的平移扰动范围（默认：1.5）
- `--batch_size`: 批次大小（默认：1）
- `--xyz_only`: 是否只使用点云XYZ坐标（默认：1）

### 4.4 推理流程详解

#### 4.4.1 模型加载

```python
# 创建模型（必须与训练时配置一致）
model = BEVCalib(
    deformable=False,      # 必须与训练时一致
    bev_encoder=True,       # 必须与训练时一致
).to(device)

# 加载检查点
ckpt = torch.load(args.ckpt_path, map_location=device)
model.load_state_dict(ckpt["model_state_dict"])
model.eval()
```

#### 4.4.2 推理循环

```python
with torch.no_grad():
    for b_idx, (imgs, pcs, masks, gt_T_to_camera, intrinsics) in enumerate(loader):
        # 1. 生成扰动
        init_T_to_camera, ang_err, trans_err = generate_single_perturbation_from_T(
            gt_T_to_camera,
            angle_range_deg=eval_angle,
            trans_range=eval_trans_range
        )
        
        # 2. 数据准备
        resize_imgs = torch.from_numpy(...).to(device)
        pcs = torch.from_numpy(...).to(device)
        ...
        
        # 3. 前向传播
        T_pred, init_loss, loss = model(
            resize_imgs, pcs, gt_T_to_camera,
            init_T_to_camera, post_cam2ego_T,
            intrinsic_matrix, masks=masks
        )
        
        # 4. 计算误差
        translation_error = torch.abs(
            T_pred[:, :3, 3] - gt_T_to_camera[:, :3, 3]
        )
        rotation_error = torch.abs(
            euler_angles(T_pred[:, :3, :3] @ gt_T_to_camera[:, :3, :3].transpose(-2, -1))
        )
        
        # 5. 记录指标
        total_losses.append(loss["total_loss"].item())
        translation_errors.append(translation_error)
        rotation_errors.append(rotation_error)
```

#### 4.4.3 误差计算

1. **平移误差**：
   ```python
   translation_error = |T_pred[:, :3, 3] - gt_T[:, :3, 3]|
   ```
   单位：米（m）

2. **旋转误差**：
   ```python
   R_diff = T_pred[:, :3, :3] @ gt_T[:, :3, :3].transpose(-2, -1)
   rotation_error = euler_angles(R_diff)  # 转换为欧拉角
   ```
   单位：度（°）

3. **重投影误差**：
   - 在损失函数中计算
   - 表示点云使用预测标定矩阵变换后的3D一致性

#### 4.4.4 结果输出

推理完成后会输出：

```
Average losses:
Total loss: X.XXXXXX
Translation loss: X.XXXXXX
Rotation loss: X.XXXXXX
Quantization loss: X.XXXXXX
Reprojection loss: X.XXXXXX

STD losses:
Total loss: X.XXXXXX
...

Errors
==================================================
Average translation xyz error: [x, y, z]
Average rotation ypr error: [roll, pitch, yaw]

STD of translation xyz error: [x, y, z]
STD of rotation ypr error: [roll, pitch, yaw]
```

### 4.5 结果分析

#### 4.5.1 损失分析

- **Total loss**: 总损失，应该尽可能小
- **Translation loss**: 平移损失，单位米，应该<0.1m
- **Rotation loss**: 旋转损失，单位度，应该<1°
- **Reprojection loss**: 重投影损失，表示3D一致性

#### 4.5.2 误差分析

- **Translation error**: 平移误差，理想情况下应该<0.05m
- **Rotation error**: 旋转误差，理想情况下应该<0.5°

#### 4.5.3 TensorBoard可视化

推理结果也会记录到TensorBoard：

```bash
tensorboard --logdir ./logs/inference
```

可以查看：
- 每个样本的损失
- 误差分布
- 不同扰动范围下的性能

## 5. 常见问题

### 5.1 训练问题

#### Q1: CUDA out of memory

**解决方案**：
- 减小批次大小（`--batch_size 8` 或 `4`）
- 减小BEV空间范围（修改`bev_settings.py`）
- 使用梯度累积

#### Q2: 损失不下降

**解决方案**：
- 检查学习率是否合适
- 检查数据加载是否正确
- 检查初始标定矩阵是否正确
- 尝试使用预训练模型

#### Q3: BEV池化错误

**解决方案**：
- 确保CUDA扩展已正确编译
- 检查CUDA版本是否匹配
- 重新编译：`cd kitti-bev-calib/img_branch/bev_pool && python setup.py build_ext --inplace`

### 5.2 推理问题

#### Q1: 模型加载失败

**解决方案**：
- 确保模型配置与训练时一致（`deformable`, `bev_encoder`）
- 检查检查点文件是否完整
- 确保PyTorch版本兼容

#### Q2: 推理结果不准确

**解决方案**：
- 检查测试时的扰动范围是否与训练时一致
- 检查数据预处理是否与训练时一致
- 尝试使用不同的检查点

### 5.3 数据问题

#### Q1: 数据集加载失败

**解决方案**：
- 检查数据集路径是否正确
- 检查数据集目录结构是否符合要求
- 确保图像和点云文件都存在

#### Q2: 标定矩阵错误

**解决方案**：
- 检查`calib.txt`文件格式
- 确保使用正确的相机（cam2）和LiDAR（velo）标定

## 6. 最佳实践

### 6.1 训练建议

1. **从小扰动开始**：如果初始标定较准确，使用较小的扰动范围（10°, 0.5m）
2. **逐步增加扰动**：随着训练进行，可以逐步增加扰动范围
3. **监控验证损失**：关注验证损失，避免过拟合
4. **保存中间检查点**：设置`--save_ckpt_per_epoches`定期保存

### 6.2 推理建议

1. **使用验证集**：在验证集上评估性能
2. **多组扰动测试**：测试不同扰动范围下的性能
3. **可视化结果**：使用TensorBoard查看详细结果
4. **对比分析**：对比不同配置下的性能

### 6.3 模型选择

- **快速训练**：`deformable=0, bev_encoder=0`
- **平衡性能**：`deformable=0, bev_encoder=1`（推荐）
- **最高精度**：`deformable=1, bev_encoder=1`

## 7. 实验记录

建议记录以下信息：

1. **训练配置**：
   - 学习率、批次大小、训练轮数
   - 扰动范围
   - 模型配置

2. **训练结果**：
   - 最终训练损失
   - 验证损失
   - 训练时间

3. **测试结果**：
   - 平移误差（x, y, z）
   - 旋转误差（roll, pitch, yaw）
   - 不同扰动范围下的性能

4. **环境信息**：
   - GPU型号和显存
   - CUDA版本
   - PyTorch版本

## 8. 总结

BEVCalib的训练和测试流程包括：

1. **环境准备**：安装依赖、编译CUDA扩展
2. **数据准备**：下载和准备KITTI数据集
3. **模型训练**：使用扰动数据训练模型，监控训练过程
4. **模型测试**：在测试集上评估模型性能

关键点：
- 确保CUDA扩展正确编译
- 使用合适的扰动范围
- 监控训练和验证损失
- 保存和加载检查点
- 详细记录实验配置和结果

# BEVCalib 代码架构文档

## 1. 项目结构

```
BEVCalib/
├── kitti-bev-calib/          # 主要代码目录
│   ├── __init__.py
│   ├── bev_calib.py          # 主模型定义
│   ├── bev_settings.py        # BEV空间配置
│   ├── train_kitti.py        # 训练脚本
│   ├── inference_kitti.py    # 推理/测试脚本
│   ├── kitti_dataset.py      # 数据集加载
│   ├── tools.py              # 工具函数
│   ├── proj_head.py          # 投影头
│   ├── img_branch/           # 图像分支
│   │   ├── img_branch.py     # 图像到BEV转换
│   │   ├── img_encoders.py   # 图像编码器
│   │   └── bev_pool/         # BEV池化操作（CUDA）
│   ├── pc_branch/            # 点云分支
│   │   ├── pc_branch.py      # 点云到BEV转换
│   │   └── pc_encoders.py    # 点云编码器
│   ├── BEVEncoder/           # BEV编码器
│   │   ├── BEVEncoder.py
│   │   ├── second.py
│   │   └── secondfpn.py
│   └── losses/               # 损失函数
│       ├── losses.py
│       └── quat_tools.py
├── ckpt/                     # 模型检查点
├── docs/                     # 文档
├── requirements.txt          # 依赖
└── README.md                 # 说明文档
```

## 2. 核心模块详解

### 2.1 主模型：`bev_calib.py`

#### 2.1.1 BEVCalib类

**功能**：整个模型的主类，整合图像分支、点云分支和标定预测。

**主要组件**：

```python
class BEVCalib(nn.Module):
    def __init__(self, num_heads=8, num_layers=2, 
                 deformable=True, bev_encoder=False):
        # 图像分支：将图像投影到BEV空间
        self.img_branch = Cam2BEV()
        
        # 点云分支：将点云投影到BEV空间
        self.pc_branch = Lidar2BEV()
        
        # BEV编码器（可选）：进一步提取BEV特征
        if bev_encoder:
            self.bev_encoder = BEVEncoder()
        
        # 特征融合器：融合图像和点云BEV特征
        self.conv_fuser = ConvFuser(...)
        
        # 位置编码：可学习的BEV位置编码
        self.pose_embed = nn.Parameter(...)
        
        # Transformer编码器：提取全局特征
        if deformable:
            self.deformable_transformer = ...
        else:
            self.transformer = ...
        
        # 标定预测头
        self.translation_pred = nn.Linear(embed_dim, 3)
        self.rotation_pred = nn.Linear(embed_dim, 4)
        
        # 损失函数
        self.loss_fn = realworld_loss()
```

**前向传播流程**：

1. **图像处理**：
   ```python
   cam2ego_T = torch.inverse(init_T_to_camera)
   cam_bev_feats, cam_bev_mask = self.img_branch(
       cam2ego_T=cam2ego_T,
       cam_intrins=cam_intrinsic,
       post_cam2ego_T=post_cam2ego_T,
       imgs=img
   )
   ```

2. **点云处理**：
   ```python
   pc = pc.permute(0, 2, 1).contiguous()  # (B, 3, N)
   pc_bev_feats = self.pc_branch(pc)    # (B, C, H, W)
   ```

3. **特征融合**：
   ```python
   x = self.conv_fuser(cam_bev_feats, pc_bev_feats)
   if self.bev_encoder_use:
       x = self.bev_encoder(x)
   x = x + self.pose_embed
   ```

4. **Transformer编码**：
   ```python
   if self.deformable:
       x = self.deformable_transformer(x)
       x = x.mean(dim=1)  # 全局池化
   else:
       x = self.transformer(masked_x, ...)
       x = x.mean(dim=1)
   ```

5. **标定预测**：
   ```python
   translation = self.translation_pred(x)  # (B, 3)
   rotation = self.rotation_pred(x)       # (B, 4)
   pred_T = self.get_T_matrix(translation, rotation)
   ```

#### 2.1.2 ConvFuser类

**功能**：融合图像和点云BEV特征。

```python
class ConvFuser(nn.Sequential):
    def __init__(self, img_in_channel, pc_in_channel, out_channel):
        super().__init__(
            nn.Conv2d(img_in_channel + pc_in_channel, out_channel, 1),
            nn.BatchNorm2d(out_channel),
            nn.ReLU(True)
        )
```

#### 2.1.3 deformable_transformer_layer类

**功能**：可变形Transformer层，用于提取BEV特征的全局信息。

```python
class deformable_transformer_layer(nn.Module):
    def __init__(self, dim=512, dim_head=64, heads=8, ...):
        self.norm1 = nn.BatchNorm2d(dim)
        self.norm2 = nn.BatchNorm2d(dim)
        self.deformable_attention = DeformableAttention(...)
        self.mlp = nn.Sequential(...)
```

### 2.2 图像分支：`img_branch/img_branch.py`

#### 2.2.1 Cam2BEV类

**功能**：将图像投影到BEV空间。

**主要组件**：

```python
class Cam2BEV(nn.Module):
    def __init__(self):
        # LSS模块：Lift-Splat-Shoot
        self.lss = LSS()
        
        # 图像编码器：Swin Transformer + FPN
        self.CamEncode = SwinT_tiny_Encoder(...)
        
        # BEV空间参数
        self.dx, self.bx, self.nx = gen_dx_bx(...)
        
        # 投影头
        self.proj_head = ProjectionHead(...)
```

**前向传播**：

1. **图像编码**：
   ```python
   imgs = (imgs - self.mean) / self.std  # 归一化
   img_feats = self.CamEncode(imgs)      # (B, N, 256, H_f, W_f)
   ```

2. **LSS投影**：
   ```python
   geometry, img_depth_feature = self.lss(
       cam2ego_rot, cam2ego_trans,
       cam_intrins,
       post_cam2ego_rot, post_cam2ego_trans,
       img_feats
   )
   ```

3. **BEV池化**：
   ```python
   bev_feats = self.bev_pool(geometry, img_depth_feature)
   ```

4. **投影头**：
   ```python
   bev_feats = self.proj_head(bev_feats)
   ```

#### 2.2.2 LSS类

**功能**：实现Lift-Splat-Shoot算法。

**Lift阶段**：

```python
def get_cam_feature(self, img_feats):
    # 深度预测
    img_feats = self.depth_net(img_feats)  # (B*N, 256+D, H, W)
    depth = img_feats[:, :self.D].softmax(dim=1)  # 深度分布
    img_feats = depth.unsqueeze(1) * img_feats[:, self.D:]
    return img_feats  # (B, N, 128, D, H, W)
```

**Splat阶段**：

```python
def get_geometry(self, cam2ego_rot, cam2ego_trans, ...):
    # 构建视锥体
    points = self.frustum - img_post_trans
    # 变换到相机坐标系
    points = torch.inverse(img_post_rots).matmul(points)
    # 变换到ego坐标系
    combine = img_rots.matmul(torch.inverse(cam_intrins))
    points = combine.matmul(points) + img_trans
    return points
```

#### 2.2.3 bev_pool方法

**功能**：将视锥体特征聚合到BEV网格。

```python
def bev_pool(self, img_depth_feature, geometry):
    # 对齐到体素网格
    geom_feats = ((geometry - (self.bx - self.dx/2)) / self.dx).long()
    
    # 过滤边界外的点
    kept = (geom_feats在有效范围内)
    
    # CUDA加速的BEV池化
    cam_bev = bev_pool(img_pc, geom_feats, B, ...)
    
    # 拼接所有Z层
    cam_bev = torch.cat(cam_bev.unbind(dim=2), 1)
    return cam_bev
```

#### 2.2.4 SwinT_tiny_Encoder类

**功能**：使用Swin Transformer提取图像特征。

```python
class SwinT_tiny_Encoder(nn.Module):
    def __init__(self):
        # 预训练的Swin Transformer
        self.model = SwinModel.from_pretrained(
            "microsoft/swin-tiny-patch4-window7-224"
        )
        # FPN用于多尺度特征融合
        self.FPN = FPN(FPN_in_channels, FPN_out_channels)
```

### 2.3 点云分支：`pc_branch/pc_branch.py`

#### 2.3.1 Lidar2BEV类

**功能**：将点云投影到BEV空间。

**主要组件**：

```python
class Lidar2BEV(nn.Module):
    def __init__(self):
        # 体素化操作
        self.ptvoxel = PointToVoxel(
            vsize_xyz=vsize_xyz,
            coors_range_xyz=(xbound, ybound, zbound),
            max_num_voxels=120000,
            max_num_points_per_voxel=10
        )
        
        # 稀疏卷积编码器
        self.sparse_encoder = SparseEncoder(sparse_shape)
        
        # 投影头
        self.proj_head = ProjectionHead()
```

**前向传播**：

1. **体素化**：
   ```python
   vox, coors, num_points = self.voxelize(pc)
   ```

2. **坐标转换**：
   ```python
   # 从ZYX转换为XYZ
   vox = torch.cat([c_part, x_part, y_part, z_part], dim=1)
   coors = coors[:, [0, 3, 2, 1]]
   ```

3. **稀疏卷积**：
   ```python
   out = self.sparse_encoder(vox, coors, B)
   ```

4. **投影头**：
   ```python
   out = self.proj_head(out)
   ```

#### 2.3.2 SparseEncoder类

**功能**：使用3D稀疏卷积提取点云特征。

**网络结构**：

```python
class SparseEncoder(nn.Module):
    def __init__(self):
        # 输入层
        self.conv_input = spconv.SparseSequential(
            spconv.SubMConv3d(3, 16, 3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU()
        )
        
        # 4层卷积，逐步下采样
        # Layer 1: 16 -> 32
        # Layer 2: 32 -> 64
        # Layer 3: 64 -> 128
        # Layer 4: 128 -> 128
        
        # 输出层：进一步下采样Z维度
        self.conv_out = spconv.SparseSequential(
            spconv.SparseConv3d(128, 128, 3, stride=(1,1,2), ...)
        )
```

**前向传播**：

```python
def forward(self, features, coors, batch_size):
    x = spconv.SparseConvTensor(features, coors, sparse_shape, batch_size)
    x = self.conv_input(x)
    
    for layer in self.conv_layers:
        for stage in layer:
            x = stage(x)
    
    x = self.conv_out(x)
    x = x.dense(False)  # 转换为密集张量
    x = x.view(B, X, Y, Z*C).permute(0, 3, 1, 2)
    return x  # (B, 128, 90, 90)
```

### 2.4 BEV编码器：`BEVEncoder/BEVEncoder.py`

#### 2.4.1 BEVEncoder类

**功能**：进一步提取和增强BEV特征。

```python
class BEVEncoder(nn.Module):
    def __init__(self):
        self.second = SECOND()      # 3D卷积编码
        self.fpn = SECONDFPN()      # 特征金字塔网络
    
    def forward(self, x):
        x = self.second(x)
        x = self.fpn(x)
        return x
```

### 2.5 损失函数：`losses/losses.py`

#### 2.5.1 realworld_loss类

**功能**：计算总损失。

**组成**：

```python
class realworld_loss(nn.Module):
    def __init__(self):
        self.translation_loss = translation_loss()
        self.rotation_loss = rotation_loss()
        self.quat_norm_loss = quat_norm_loss()
        self.PC_reproj_loss = PC_reproj_loss()
    
    def forward(self, pred_translation, pred_rotation, 
                pcs, gt_T_to_camera, init_T_to_camera, mask):
        # 构建预测的变换矩阵
        T_pred = batch_tvector2mat(pred_translation)
        R_pred = batch_quat2mat(pred_rotation)
        T_pred = torch.bmm(T_pred, R_pred)
        
        # 计算期望的GT变换
        T_gt_expected = torch.matmul(T_pred.inverse(), init_T_to_camera)
        
        # 计算各项损失
        translation_loss = self.translation_loss(...)
        rotation_loss = self.rotation_loss(...)
        PC_reproj_loss = self.PC_reproj_loss(...)
        quat_norm_loss = self.quat_norm_loss(pred_rotation)
        
        # 总损失
        total_loss = w_t * translation_loss + 
                     w_r * rotation_loss + 
                     w_p * PC_reproj_loss + 
                     w_q * quat_norm_loss
        
        return loss_dict, T_gt_expected
```

#### 2.5.2 各项损失函数

**平移损失**：
```python
class translation_loss(nn.Module):
    def forward(self, pred_translation, gt_translation):
        return SmoothL1Loss(pred_translation, gt_translation)
```

**旋转损失**：
```python
class rotation_loss(nn.Module):
    def forward(self, pred_rotation, gt_rotation):
        # 转换为四元数
        pred_quat = quaternion_from_matrix(pred_rotation)
        gt_quat = quaternion_from_matrix(gt_rotation)
        # 计算四元数距离
        return quaternion_distance(pred_quat, gt_quat)
```

**重投影损失**：
```python
class PC_reproj_loss(nn.Module):
    def forward(self, pcs, gt_T_to_camera, pred_translation, 
                pred_rotation, mask):
        # 使用预测的变换矩阵变换点云
        points_transformed = T_total * points_homogeneous
        # 计算与原始点云的距离
        error = (points_transformed - pc).norm(dim=1)
        return error.mean()
```

### 2.6 数据集：`kitti_dataset.py`

#### 2.6.1 KittiDataset类

**功能**：加载KITTI数据集。

```python
class KittiDataset(Dataset):
    def __init__(self, data_folder):
        # 加载所有序列
        for seq in sequences:
            odom = odometry(data_folder, seq)
            # 提取标定参数
            self.K[seq] = calib.K_cam2  # 相机内参
            self.T[seq] = T_velo2_cam0  # 标定矩阵
            # 收集所有图像-点云对
            self.all_files.append(...)
    
    def __getitem__(self, idx):
        # 加载图像
        img = Image.open(img_path)
        
        # 加载点云
        pcd = np.fromfile(pcd_path, dtype=np.float32)
        # 过滤近距离点
        pcd = pcd[valid_ind, :]
        
        return img, pcd, gt_transform, intrinsic
```

### 2.7 训练脚本：`train_kitti.py`

#### 2.7.1 主要流程

```python
def main():
    # 1. 解析参数
    args = parse_args()
    
    # 2. 创建数据集和数据加载器
    dataset = KittiDataset(dataset_root)
    train_dataset, val_dataset = random_split(dataset, [0.8, 0.2])
    train_loader = DataLoader(...)
    val_loader = DataLoader(...)
    
    # 3. 创建模型
    model = BEVCalib(...)
    
    # 4. 创建优化器和调度器
    optimizer = torch.optim.AdamW(...)
    scheduler = StepLR(...)
    
    # 5. 训练循环
    for epoch in range(num_epochs):
        for batch in train_loader:
            # 生成扰动后的初始标定矩阵
            init_T = generate_single_perturbation_from_T(
                gt_T, angle_range_deg, trans_range
            )
            
            # 前向传播
            T_pred, init_loss, loss = model(...)
            
            # 反向传播
            total_loss.backward()
            optimizer.step()
        
        # 验证
        if epoch % eval_epoches == 0:
            validate(...)
```

#### 2.7.2 数据预处理

```python
def collate_fn(batch):
    # 图像裁剪和缩放
    processed_data = [crop_and_resize(...) for item in batch]
    
    # 点云填充到相同长度
    for item in batch:
        if pc.shape[0] < max_num_points:
            pc = np.concatenate([pc, padding], axis=0)
    
    return imgs, pcs, masks, gt_T_to_camera, intrinsics
```

### 2.8 推理脚本：`inference_kitti.py`

#### 2.8.1 主要流程

```python
def main():
    # 1. 加载模型
    model = BEVCalib(...)
    ckpt = torch.load(ckpt_path)
    model.load_state_dict(ckpt["model_state_dict"])
    model.eval()
    
    # 2. 推理循环
    with torch.no_grad():
        for batch in loader:
            # 生成扰动
            init_T, ang_err, trans_err = generate_single_perturbation_from_T(...)
            
            # 前向传播
            T_pred, _, loss = model(...)
            
            # 计算误差
            translation_error = |T_pred[:, :3, 3] - gt_T[:, :3, 3]|
            rotation_error = euler_angles(T_pred @ gt_T.T)
            
            # 记录指标
            metrics.append(...)
    
    # 3. 输出统计结果
    print("Average losses:", ...)
    print("Average errors:", ...)
```

### 2.9 工具函数：`tools.py`

#### 2.9.1 generate_single_perturbation_from_T

**功能**：生成扰动后的标定矩阵。

```python
def generate_single_perturbation_from_T(T, angle_range_deg, trans_range):
    for i in range(B):
        # 提取原始旋转和平移
        orig_rot = R.from_matrix(T[i, :3, :3])
        orig_trans = T[i, :3, 3]
        
        # 生成随机旋转扰动
        rand_axis = np.random.randn(3)
        rand_angle = np.deg2rad(np.random.uniform(-angle_range, angle_range))
        delta_rot = R.from_rotvec(rand_angle * rand_axis)
        new_rot = delta_rot * orig_rot
        
        # 生成随机平移扰动
        rand_direction = np.random.randn(3)
        delta_trans = rand_direction * np.random.uniform(0, trans_range)
        new_trans = orig_trans + delta_trans
        
        # 构建新的变换矩阵
        T_new[i] = construct_T(new_rot, new_trans)
    
    return T_new, ang_err, trans_err
```

## 3. 数据流

### 3.1 训练时数据流

```
原始数据
  ↓
KittiDataset.__getitem__()
  ↓
collate_fn() - 批处理和预处理
  ↓
generate_single_perturbation_from_T() - 生成扰动
  ↓
BEVCalib.forward()
  ├─→ Cam2BEV() - 图像 → BEV
  │   ├─→ SwinT_tiny_Encoder() - 图像编码
  │   ├─→ LSS() - Lift-Splat-Shoot
  │   └─→ bev_pool() - BEV池化
  │
  ├─→ Lidar2BEV() - 点云 → BEV
  │   ├─→ voxelize() - 体素化
  │   └─→ SparseEncoder() - 稀疏卷积
  │
  ├─→ ConvFuser() - 特征融合
  ├─→ BEVEncoder() - BEV编码（可选）
  ├─→ Transformer - 全局特征提取
  └─→ Linear Heads - 标定预测
  ↓
realworld_loss() - 损失计算
  ↓
反向传播和优化
```

### 3.2 推理时数据流

```
测试数据
  ↓
KittiDataset.__getitem__()
  ↓
collate_fn()
  ↓
generate_single_perturbation_from_T() - 生成扰动
  ↓
BEVCalib.forward() (eval模式)
  ↓
预测的标定矩阵
  ↓
误差计算和指标统计
```

## 4. 关键配置

### 4.1 BEV空间配置（`bev_settings.py`）

```python
# BEV范围
xbound = (-90.0, 90.0, 2.0)  # X: -90m到90m，分辨率2m
ybound = (-90.0, 90.0, 2.0)  # Y: -90m到90m，分辨率2m
zbound = (-10.0, 10.0, 20.0) # Z: -10m到10m，分辨率20m

# 深度配置
d_conf = (1.0, 90.0, 1.0)    # 深度范围：1m到90m，步长1m

# 稀疏形状（用于点云体素化）
sparse_shape = (720, 720, 41)  # 对应BEV形状的8倍下采样
```

### 4.2 模型配置

```python
# 默认配置
num_heads = 8              # Transformer注意力头数
num_layers = 2             # Transformer层数（可变形）或8层（标准）
deformable = True          # 是否使用可变形注意力
bev_encoder = False        # 是否使用BEV编码器
embed_dim = 256            # 特征维度（图像128 + 点云128）
```

### 4.3 训练配置

```python
# 默认训练参数
batch_size = 16
num_epochs = 500
lr = 1e-4
weight_decay = 1e-4
angle_range_deg = 20.0     # 旋转扰动范围
trans_range = 1.5          # 平移扰动范围
```

## 5. 依赖关系

### 5.1 核心依赖

- **PyTorch**: 深度学习框架
- **spconv**: 稀疏卷积库（用于点云处理）
- **transformers**: Swin Transformer模型
- **deformable_attention**: 可变形注意力机制
- **opencv-python**: 图像处理
- **open3d**: 点云处理
- **pykitti**: KITTI数据集加载

### 5.2 CUDA扩展

- **bev_pool**: BEV池化操作的CUDA实现
  - 位置：`kitti-bev-calib/img_branch/bev_pool/`
  - 需要编译：`python setup.py build_ext --inplace`

## 6. 扩展性

### 6.1 添加新的数据集

1. 创建新的Dataset类（参考`KittiDataset`）
2. 实现`__getitem__`方法返回图像、点云、标定矩阵和内参
3. 在训练/推理脚本中使用新的数据集类

### 6.2 修改BEV空间配置

修改`bev_settings.py`中的参数：
- `xbound`, `ybound`, `zbound`: BEV范围
- `d_conf`: 深度配置
- `sparse_shape`: 点云体素化形状

### 6.3 自定义损失函数

在`losses/losses.py`中添加新的损失函数，并在`realworld_loss`中集成。

## 7. 性能优化

### 7.1 内存优化

- 使用混合精度训练（FP16）
- 减小批次大小
- 减小BEV空间范围

### 7.2 速度优化

- 使用CUDA加速的BEV池化
- 使用稀疏卷积处理点云
- 减少Transformer层数

## 8. 调试技巧

### 8.1 检查BEV特征

```python
# 可视化BEV特征图
import matplotlib.pyplot as plt
bev_feat = cam_bev_feats[0, 0].cpu().numpy()
plt.imshow(bev_feat)
plt.show()
```

### 8.2 检查标定矩阵

```python
# 打印预测的标定矩阵
print("Predicted T:", T_pred)
print("GT T:", gt_T)
print("Error:", torch.norm(T_pred - gt_T))
```

### 8.3 检查损失

```python
# 分别检查各项损失
print("Translation loss:", loss["translation_loss"])
print("Rotation loss:", loss["rotation_loss"])
print("Reproj loss:", loss["PC_reproj_loss"])
```

## 9. 总结

BEVCalib的代码架构清晰，主要分为以下几个部分：

1. **数据加载**：`KittiDataset`负责加载图像和点云数据
2. **图像分支**：`Cam2BEV`使用LSS将图像投影到BEV空间
3. **点云分支**：`Lidar2BEV`使用稀疏卷积将点云投影到BEV空间
4. **特征融合**：`ConvFuser`融合两种模态的特征
5. **标定预测**：Transformer提取全局特征，线性层预测标定参数
6. **损失计算**：`realworld_loss`计算多项损失

整个架构设计合理，模块化程度高，易于扩展和修改。

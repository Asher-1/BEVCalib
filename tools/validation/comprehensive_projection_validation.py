#!/usr/bin/env python3
"""
å…¨é¢çš„ç‚¹äº‘æŠ•å½±éªŒè¯å·¥å…·
å¯¹æ¯ä¸ªåºåˆ—é‡‡æ ·å¤šä¸ªå…³é”®å¸§è¿›è¡ŒæŠ•å½±éªŒè¯ï¼Œå¹¶æŒ‰åºåˆ—åˆ†ç±»å­˜å‚¨
"""

import matplotlib
matplotlib.use('Agg')

import numpy as np
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
import json
from datetime import datetime


def load_calib(calib_file):
    """åŠ è½½æ ‡å®šæ–‡ä»¶"""
    calib = {}
    with open(calib_file, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            key, values = line.split(':', 1)
            key = key.strip()
            values = values.strip()
            
            try:
                calib[key] = np.array([float(x) for x in values.split()])
            except ValueError:
                calib[key] = values
    return calib


def load_pointcloud(bin_file):
    """åŠ è½½ç‚¹äº‘æ–‡ä»¶"""
    points = np.fromfile(bin_file, dtype=np.float32)
    points = points.reshape(-1, 4)
    return points


def project_points(points, calib, img_shape):
    """æŠ•å½±ç‚¹äº‘åˆ°å›¾åƒ"""
    # è·å–æ ‡å®šå‚æ•°
    Tr_3x4 = calib['Tr'].reshape(3, 4)
    Tr = np.vstack([Tr_3x4, [0, 0, 0, 1]])
    P2 = calib['P2'].reshape(3, 4)
    
    # è½¬æ¢ä¸ºé½æ¬¡åæ ‡
    points_hom = np.hstack([points[:, :3], np.ones((points.shape[0], 1))])
    
    # å˜æ¢åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆä½¿ç”¨inv(Tr)ï¼‰
    Tr_inv = np.linalg.inv(Tr)
    points_cam = (Tr_inv @ points_hom.T).T[:, :3]
    
    # è¿‡æ»¤æ‰ç›¸æœºåé¢çš„ç‚¹
    mask = points_cam[:, 2] > 0
    points_cam = points_cam[mask]
    depths = points_cam[:, 2].copy()
    
    if len(points_cam) == 0:
        return None, None, 0
    
    # æŠ•å½±åˆ°å›¾åƒ
    points_cam_hom = np.hstack([points_cam, np.ones((points_cam.shape[0], 1))])
    points_img = (P2 @ points_cam_hom.T).T
    points_img = points_img[:, :2] / points_img[:, 2:3]
    
    # è¿‡æ»¤å›¾åƒå†…çš„ç‚¹
    H, W = img_shape[:2]
    mask = (points_img[:, 0] >= 0) & (points_img[:, 0] < W) & \
           (points_img[:, 1] >= 0) & (points_img[:, 1] < H)
    
    points_img = points_img[mask]
    depths = depths[mask]
    
    return points_img, depths, mask.sum()


def visualize_projection(dataset_root, sequence, frame, output_file):
    """å¯è§†åŒ–ç‚¹äº‘æŠ•å½±"""
    dataset_root = Path(dataset_root)
    seq_dir = dataset_root / 'sequences' / sequence
    
    # åŠ è½½æ•°æ®
    img_file = seq_dir / 'image_2' / f'{frame:06d}.png'
    pc_file = seq_dir / 'velodyne' / f'{frame:06d}.bin'
    calib_file = seq_dir / 'calib.txt'
    
    if not img_file.exists() or not pc_file.exists() or not calib_file.exists():
        print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¸§ {frame}")
        return None
    
    # åŠ è½½æ•°æ®
    img = cv2.imread(str(img_file))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    points = load_pointcloud(pc_file)
    calib = load_calib(calib_file)
    
    # æŠ•å½±
    points_img, depths, num_visible = project_points(points, calib, img.shape)
    
    if num_visible == 0:
        print(f"  âŒ å¸§ {frame}: æ²¡æœ‰å¯è§ç‚¹")
        return None
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, ax = plt.subplots(1, 1, figsize=(19.2, 10.8), dpi=100)
    ax.imshow(img)
    
    # æ·±åº¦ç€è‰²
    scatter = ax.scatter(points_img[:, 0], points_img[:, 1],
                        c=depths, cmap='jet', s=1, alpha=0.5)
    plt.colorbar(scatter, ax=ax, label='Depth (m)')
    
    ax.set_title(f'Sequence {sequence} - Frame {frame:06d}\n'
                f'Visible Points: {num_visible}/{points.shape[0]} ({100*num_visible/points.shape[0]:.1f}%)',
                fontsize=14)
    ax.axis('off')
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    plt.close()
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯
    return {
        'frame': frame,
        'total_points': int(points.shape[0]),
        'visible_points': int(num_visible),
        'visible_ratio': float(num_visible / points.shape[0]),
        'depth_min': float(depths.min()),
        'depth_max': float(depths.max()),
        'depth_mean': float(depths.mean()),
        'output_file': str(output_file)
    }


def get_sample_frames(num_frames, num_samples=10):
    """è·å–é‡‡æ ·å¸§ç´¢å¼•"""
    if num_frames <= num_samples:
        return list(range(num_frames))
    
    # é‡‡æ ·ç­–ç•¥ï¼šå‡åŒ€åˆ†å¸ƒ10ä¸ªé‡‡æ ·ç‚¹
    # åŒ…å«å¼€å§‹å’Œç»“æŸï¼Œä¸­é—´8ä¸ªç‚¹å‡åŒ€åˆ†å¸ƒ
    indices = []
    for i in range(num_samples):
        idx = int(i * (num_frames - 1) / (num_samples - 1))
        indices.append(idx)
    
    return sorted(list(set(indices)))  # å»é‡å¹¶æ’åº


def validate_sequence(dataset_root, sequence, output_base_dir):
    """éªŒè¯å•ä¸ªåºåˆ—çš„æŠ•å½±æ•ˆæœ"""
    dataset_root = Path(dataset_root)
    seq_dir = dataset_root / 'sequences' / sequence
    image_dir = seq_dir / 'image_2'
    
    if not image_dir.exists():
        print(f"  âŒ åºåˆ— {sequence}: å›¾åƒç›®å½•ä¸å­˜åœ¨")
        return None
    
    # è·å–å¸§æ•°
    images = sorted(image_dir.glob('*.png'))
    num_frames = len(images)
    
    if num_frames == 0:
        print(f"  âŒ åºåˆ— {sequence}: æ²¡æœ‰å›¾åƒæ–‡ä»¶")
        return None
    
    print(f"\n{'='*80}")
    print(f"éªŒè¯åºåˆ— {sequence} ({num_frames} å¸§)")
    print(f"{'='*80}")
    
    # åˆ›å»ºåºåˆ—è¾“å‡ºç›®å½•
    seq_output_dir = output_base_dir / f'sequence_{sequence}'
    seq_output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–é‡‡æ ·å¸§
    sample_frames = get_sample_frames(num_frames, num_samples=10)
    print(f"é‡‡æ ·å¸§: {sample_frames}")
    print(f"è¾“å‡ºç›®å½•: {seq_output_dir}")
    
    # å¯¹æ¯ä¸ªé‡‡æ ·å¸§è¿›è¡ŒæŠ•å½±
    results = []
    for frame_idx in sample_frames:
        output_file = seq_output_dir / f'frame_{frame_idx:06d}.png'
        
        print(f"  å¤„ç†å¸§ {frame_idx:06d}...", end=' ')
        
        result = visualize_projection(dataset_root, sequence, frame_idx, output_file)
        
        if result:
            print(f"âœ“ ({result['visible_points']}/{result['total_points']} ç‚¹, {result['visible_ratio']*100:.1f}%)")
            results.append(result)
        else:
            print("è·³è¿‡")
    
    # ä¿å­˜åºåˆ—ç»Ÿè®¡
    if results:
        stats = {
            'sequence': sequence,
            'num_frames': num_frames,
            'num_samples': len(results),
            'sample_frames': sample_frames,
            'results': results,
            'summary': {
                'avg_visible_ratio': sum(r['visible_ratio'] for r in results) / len(results),
                'avg_depth': sum(r['depth_mean'] for r in results) / len(results),
                'min_depth': min(r['depth_min'] for r in results),
                'max_depth': max(r['depth_max'] for r in results)
            }
        }
        
        stats_file = seq_output_dir / 'statistics.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"\n  âœ… åºåˆ— {sequence} éªŒè¯å®Œæˆ: {len(results)}/{len(sample_frames)} å¸§æˆåŠŸ")
        print(f"  å¹³å‡å¯è§ç‡: {stats['summary']['avg_visible_ratio']*100:.1f}%")
        
        return stats
    
    return None


def generate_overview_report(all_results, output_dir):
    """ç”Ÿæˆæ€»è§ˆæŠ¥å‘Š"""
    report_file = output_dir / 'PROJECTION_VALIDATION_REPORT.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç‚¹äº‘æŠ•å½±éªŒè¯æŠ¥å‘Š\n\n")
        f.write(f"**éªŒè¯æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        f.write("## æ€»ä½“æ¦‚å†µ\n\n")
        f.write(f"- éªŒè¯åºåˆ—æ•°: {len(all_results)}\n")
        total_samples = sum(r['num_samples'] for r in all_results)
        f.write(f"- æ€»é‡‡æ ·å¸§æ•°: {total_samples}\n")
        f.write(f"- æ¯åºåˆ—é‡‡æ ·: 10 å¸§ (å‡åŒ€åˆ†å¸ƒ)\n\n")
        
        f.write("---\n\n")
        f.write("## å„åºåˆ—æŠ•å½±æ•ˆæœ\n\n")
        f.write("| åºåˆ— | æ€»å¸§æ•° | é‡‡æ ·æ•° | å¹³å‡å¯è§ç‡ | æ·±åº¦èŒƒå›´ | çŠ¶æ€ |\n")
        f.write("|------|--------|--------|-----------|----------|------|\n")
        
        for result in sorted(all_results, key=lambda x: x['sequence']):
            seq = result['sequence']
            num_frames = result['num_frames']
            num_samples = result['num_samples']
            avg_ratio = result['summary']['avg_visible_ratio'] * 100
            min_depth = result['summary']['min_depth']
            max_depth = result['summary']['max_depth']
            
            f.write(f"| {seq} | {num_frames:,} | {num_samples} | {avg_ratio:.1f}% | "
                   f"{min_depth:.1f}-{max_depth:.1f}m | âœ… |\n")
        
        f.write("\n---\n\n")
        f.write("## è¯¦ç»†ç»“æœ\n\n")
        
        for result in sorted(all_results, key=lambda x: x['sequence']):
            seq = result['sequence']
            f.write(f"### åºåˆ— {seq}\n\n")
            f.write(f"- **æ€»å¸§æ•°**: {result['num_frames']:,}\n")
            f.write(f"- **é‡‡æ ·å¸§**: {result['sample_frames']}\n")
            f.write(f"- **å¹³å‡å¯è§ç‡**: {result['summary']['avg_visible_ratio']*100:.1f}%\n")
            f.write(f"- **æ·±åº¦èŒƒå›´**: {result['summary']['min_depth']:.1f}m - {result['summary']['max_depth']:.1f}m\n")
            f.write(f"- **å›¾åƒä½ç½®**: `sequence_{seq}/`\n\n")
            
            f.write("| å¸§ | æ€»ç‚¹æ•° | å¯è§ç‚¹ | å¯è§ç‡ | æ·±åº¦èŒƒå›´ | å›¾åƒ |\n")
            f.write("|----|--------|--------|--------|----------|------|\n")
            
            for r in result['results']:
                frame = r['frame']
                total = r['total_points']
                visible = r['visible_points']
                ratio = r['visible_ratio'] * 100
                depth_min = r['depth_min']
                depth_max = r['depth_max']
                
                f.write(f"| {frame:06d} | {total:,} | {visible:,} | {ratio:.1f}% | "
                       f"{depth_min:.1f}-{depth_max:.1f}m | `frame_{frame:06d}.png` |\n")
            
            f.write("\n")
        
        f.write("---\n\n")
        f.write("## æ–‡ä»¶ç»“æ„\n\n")
        f.write("```\n")
        f.write("projection_validation/\n")
        f.write("â”œâ”€â”€ PROJECTION_VALIDATION_REPORT.md  # æœ¬æŠ¥å‘Š\n")
        f.write("â”œâ”€â”€ summary.json                      # JSONæ ¼å¼æ±‡æ€»\n")
        
        for result in sorted(all_results, key=lambda x: x['sequence']):
            seq = result['sequence']
            f.write(f"â”œâ”€â”€ sequence_{seq}/\n")
            f.write(f"â”‚   â”œâ”€â”€ statistics.json            # åºåˆ—ç»Ÿè®¡\n")
            for r in result['results']:
                f.write(f"â”‚   â”œâ”€â”€ frame_{r['frame']:06d}.png\n")
        
        f.write("```\n\n")
        
        f.write("---\n\n")
        f.write("## æ•°æ®æ¥æºç¡®è®¤\n\n")
        f.write("âœ… **æ‰€æœ‰æŠ•å½±ä½¿ç”¨çš„æ•°æ®å‡æ¥è‡ª**:\n\n")
        f.write("- **å›¾åƒ**: `sequences/{seq_id}/image_2/{frame:06d}.png`\n")
        f.write("- **ç‚¹äº‘**: `sequences/{seq_id}/velodyne/{frame:06d}.bin`\n")
        f.write("- **æ ‡å®š**: `sequences/{seq_id}/calib.txt`\n\n")
        f.write("æ•°æ®æºä½ç½®: `/mnt/drtraining/user/dahailu/data/bevcalib/all_training_data/sequences/`\n\n")
    
    print(f"\nâœ… æ€»è§ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    parser = argparse.ArgumentParser(description='å…¨é¢çš„ç‚¹äº‘æŠ•å½±éªŒè¯')
    parser.add_argument('--dataset_root', type=str, required=True,
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--sequences', type=str, nargs='+',
                       help='æŒ‡å®šè¦éªŒè¯çš„åºåˆ— (é»˜è®¤: å…¨éƒ¨)')
    
    args = parser.parse_args()
    
    dataset_root = Path(args.dataset_root)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–è¦éªŒè¯çš„åºåˆ—
    sequences_dir = dataset_root / 'sequences'
    if args.sequences:
        sequences = args.sequences
    else:
        sequences = sorted([d.name for d in sequences_dir.iterdir() if d.is_dir()])
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š å…¨é¢ç‚¹äº‘æŠ•å½±éªŒè¯")
    print(f"{'='*80}")
    print(f"æ•°æ®é›†: {dataset_root}")
    print(f"åºåˆ—æ•°: {len(sequences)}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"{'='*80}")
    
    # éªŒè¯æ¯ä¸ªåºåˆ—
    all_results = []
    for seq in sequences:
        result = validate_sequence(dataset_root, seq, output_dir)
        if result:
            all_results.append(result)
    
    # ä¿å­˜æ±‡æ€»JSON
    summary_file = output_dir / 'summary.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'dataset_root': str(dataset_root),
            'total_sequences': len(all_results),
            'sequences': all_results
        }, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_overview_report(all_results, output_dir)
    
    print(f"\n{'='*80}")
    print(f"âœ… éªŒè¯å®Œæˆ!")
    print(f"{'='*80}")
    print(f"éªŒè¯åºåˆ—: {len(all_results)}/{len(sequences)}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æŸ¥çœ‹æŠ¥å‘Š: {output_dir}/PROJECTION_VALIDATION_REPORT.md")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    main()

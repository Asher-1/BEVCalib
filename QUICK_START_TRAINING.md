# ğŸš€ BEVCalib è®­ç»ƒå¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“‹ å‰ææ¡ä»¶

### 1. æ•°æ®é›†å‡†å¤‡

ç¡®ä¿æ•°æ®é›†å·²å‡†å¤‡å¥½å¹¶ç¬¦åˆ KITTI-Odometry æ ¼å¼ï¼š

```bash
# B26A æ•°æ®é›†
/mnt/drtraining/user/dahailu/data/bevcalib/bevcalib_training_data/

# å…¨é‡æ•°æ®é›†
/mnt/drtraining/user/dahailu/data/bevcalib/all_training_data/

# æ•°æ®é›†ç»“æ„
dataset_root/
â”œâ”€â”€ sequences/
â”‚   â”œâ”€â”€ 00/
â”‚   â”‚   â”œâ”€â”€ image_2/
â”‚   â”‚   â”œâ”€â”€ velodyne/
â”‚   â”‚   â””â”€â”€ calib.txt
â”‚   â”œâ”€â”€ 01/
â”‚   â””â”€â”€ ...
â””â”€â”€ poses/
    â”œâ”€â”€ 00.txt
    â”œâ”€â”€ 01.txt
    â””â”€â”€ ...
```

### 2. ç¯å¢ƒå‡†å¤‡

```bash
# æ¿€æ´» conda ç¯å¢ƒ
conda activate bevcalib

# æ£€æŸ¥ç¯å¢ƒ
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
nvidia-smi
```

## ğŸ¯ æœ€ç®€å•çš„æ–¹å¼ï¼šä½¿ç”¨ start_training.sh

### B26A æ•°æ®é›†è®­ç»ƒ

```bash
cd /mnt/drtraining/user/dahailu/code/BEVCalib

# å¯åŠ¨è®­ç»ƒï¼ˆç‰ˆæœ¬ v1ï¼‰
bash start_training.sh B26A v1
```

**è¯´æ˜**:
- è‡ªåŠ¨å¯åŠ¨ 2 ä¸ªè®­ç»ƒè¿›ç¨‹ï¼ˆGPU 0 å’Œ GPU 1ï¼‰
- GPU 0: 10Â° æ‰°åŠ¨ï¼Œ0.5m å¹³ç§»
- GPU 1: 5Â° æ‰°åŠ¨ï¼Œ0.3m å¹³ç§»
- æ—¥å¿—ä½ç½®: `./logs/B26A/model_*_v1/`

### all_training_data æ•°æ®é›†è®­ç»ƒ

```bash
cd /mnt/drtraining/user/dahailu/code/BEVCalib

# å¯åŠ¨è®­ç»ƒï¼ˆç‰ˆæœ¬ v1ï¼‰
bash start_training.sh all v1
```

**è¯´æ˜**:
- è‡ªåŠ¨å¯åŠ¨ 2 ä¸ªè®­ç»ƒè¿›ç¨‹ï¼ˆGPU 0 å’Œ GPU 1ï¼‰
- GPU 0: 10Â° æ‰°åŠ¨ï¼Œ0.5m å¹³ç§»
- GPU 1: 5Â° æ‰°åŠ¨ï¼Œ0.3m å¹³ç§»
- æ—¥å¿—ä½ç½®: `./logs/all_training_data/model_*_v1/`

### è‡ªå®šä¹‰æ•°æ®é›†è®­ç»ƒ

```bash
cd /mnt/drtraining/user/dahailu/code/BEVCalib

# è®¾ç½®æ•°æ®é›†è·¯å¾„å¹¶å¯åŠ¨
CUSTOM_DATASET=/path/to/your/dataset bash start_training.sh custom v1
```

## ğŸ“Š æŸ¥çœ‹è®­ç»ƒçŠ¶æ€

### å®æ—¶ç›‘æ§

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# æŸ¥çœ‹è®­ç»ƒè¿›ç¨‹
ps aux | grep train_kitti

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼ˆB26A æ•°æ®é›†ï¼‰
tail -f logs/B26A/model_small_10deg_v1/train.log

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼ˆall_training_data æ•°æ®é›†ï¼‰
tail -f logs/all_training_data/model_small_10deg_v1/train.log
```

### TensorBoard å¯è§†åŒ–

```bash
# æŸ¥çœ‹æ‰€æœ‰è®­ç»ƒ
tensorboard --logdir logs/ --port 6006

# åªæŸ¥çœ‹ B26A æ•°æ®é›†
tensorboard --logdir logs/B26A/ --port 6006

# åªæŸ¥çœ‹ all_training_data æ•°æ®é›†
tensorboard --logdir logs/all_training_data/ --port 6007
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—®: `http://localhost:6006`

## ğŸ›‘ åœæ­¢è®­ç»ƒ

```bash
# ä½¿ç”¨åœæ­¢è„šæœ¬
bash stop_training.sh

# æˆ–æ‰‹åŠ¨åœæ­¢
pkill -f train_kitti
```

## ğŸ”§ é«˜çº§ç”¨æ³•ï¼šå•ç‹¬è®­ç»ƒ

### ä½¿ç”¨ train_universal.sh

```bash
# å•ä¸ª GPU è®­ç»ƒï¼Œ10Â° æ‰°åŠ¨
bash train_universal.sh scratch \
  --dataset_root /mnt/drtraining/user/dahailu/data/bevcalib/all_training_data \
  --dataset_name all_training_data \
  --cuda_device 0 \
  --angle_range_deg 10 \
  --trans_range 0.5 \
  --log_suffix small_10deg_v1

# æ—¥å¿—ä½ç½®: logs/all_training_data/model_small_10deg_v1/
```

### å®Œæ•´å‚æ•°è¯´æ˜

```bash
bash train_universal.sh [mode] [options]

# æ¨¡å¼ (mode):
#   scratch   - ä»å¤´è®­ç»ƒï¼ˆé»˜è®¤ï¼‰
#   finetune  - ä» KITTI é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ
#   resume    - ä»æœ€åçš„æ£€æŸ¥ç‚¹æ¢å¤

# é€‰é¡¹ (options):
#   --dataset_root PATH     - æ•°æ®é›†æ ¹ç›®å½•ï¼ˆå¿…éœ€ï¼‰
#   --dataset_name NAME     - æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰
#   --cuda_device ID        - CUDA è®¾å¤‡ IDï¼ˆå¦‚ 0, 1, 2ï¼‰
#   --tensorboard_port PORT - TensorBoard ç«¯å£ï¼ˆé»˜è®¤ 6006ï¼‰
#   --log_suffix SUFFIX     - æ—¥å¿—ç›®å½•åç¼€
#   --angle_range_deg DEG   - æ—‹è½¬æ‰°åŠ¨èŒƒå›´ï¼ˆé»˜è®¤ 20ï¼‰
#   --trans_range M         - å¹³ç§»æ‰°åŠ¨èŒƒå›´ï¼ˆé»˜è®¤ 1.5ï¼‰
```

### ç¤ºä¾‹ï¼šä¸åŒæ‰°åŠ¨çº§åˆ«

```bash
# å°æ‰°åŠ¨ (5Â°, 0.3m) - é€‚åˆå·²æ ‡å®šæ•°æ®çš„å¾®è°ƒ
bash train_universal.sh scratch \
  --dataset_root /path/to/data \
  --cuda_device 0 \
  --angle_range_deg 5 \
  --trans_range 0.3 \
  --log_suffix small_5deg_v1

# ä¸­ç­‰æ‰°åŠ¨ (10Â°, 0.5m) - æ¨èç”¨äºåˆå§‹æ ‡å®š
bash train_universal.sh scratch \
  --dataset_root /path/to/data \
  --cuda_device 1 \
  --angle_range_deg 10 \
  --trans_range 0.5 \
  --log_suffix medium_10deg_v1

# å¤§æ‰°åŠ¨ (20Â°, 1.5m) - é€‚åˆå¤§è¯¯å·®æ ‡å®š
bash train_universal.sh scratch \
  --dataset_root /path/to/data \
  --cuda_device 2 \
  --angle_range_deg 20 \
  --trans_range 1.5 \
  --log_suffix large_20deg_v1
```

## ğŸ“‚ æ—¥å¿—ç›®å½•ç»“æ„

è®­ç»ƒåï¼Œæ—¥å¿—æŒ‰æ•°æ®é›†åˆ†çº§ç»„ç»‡ï¼š

```
logs/
â”œâ”€â”€ B26A/                           # B26A æ•°æ®é›†
â”‚   â”œâ”€â”€ model_small_10deg_v1/
â”‚   â”‚   â”œâ”€â”€ train.log               # è®­ç»ƒæ—¥å¿—
â”‚   â”‚   â”œâ”€â”€ events.out.tfevents.*   # TensorBoard äº‹ä»¶
â”‚   â”‚   â”œâ”€â”€ epoch_40.pth            # æ£€æŸ¥ç‚¹
â”‚   â”‚   â”œâ”€â”€ epoch_80.pth
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ model_small_5deg_v1/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ all_training_data/              # å…¨é‡æ•°æ®é›†
â”‚   â”œâ”€â”€ model_small_10deg_v1/
â”‚   â””â”€â”€ model_small_5deg_v1/
â”‚
â””â”€â”€ README.md
```

## ğŸ“ å…¸å‹å·¥ä½œæµ

### 1. å¿«é€ŸéªŒè¯ï¼ˆB26A å°æ•°æ®é›†ï¼‰

```bash
# ä½¿ç”¨å°æ•°æ®é›†å¿«é€ŸéªŒè¯
bash start_training.sh B26A v1

# æŸ¥çœ‹è®­ç»ƒè¿›åº¦
tail -f logs/B26A/model_small_10deg_v1/train.log

# è®­ç»ƒ 50-100 ä¸ª epoch åæ£€æŸ¥ç»“æœ
tensorboard --logdir logs/B26A/ --port 6006
```

### 2. å®Œæ•´è®­ç»ƒï¼ˆall_training_data å…¨é‡æ•°æ®ï¼‰

```bash
# ä½¿ç”¨å…¨é‡æ•°æ®é›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹
bash start_training.sh all v1

# ç›‘æ§è®­ç»ƒï¼ˆéœ€è¦æ›´é•¿æ—¶é—´ï¼‰
tail -f logs/all_training_data/model_small_10deg_v1/train.log

# è®­ç»ƒ 200-500 ä¸ª epoch
tensorboard --logdir logs/all_training_data/ --port 6007
```

### 3. å¯¹æ¯”å®éªŒ

```bash
# åŒæ—¶è®­ç»ƒä¸¤ä¸ªæ•°æ®é›†å¯¹æ¯”
bash start_training.sh B26A v1      # åå°è¿è¡Œ
bash start_training.sh all v2       # åå°è¿è¡Œ

# TensorBoard åŒæ—¶æŸ¥çœ‹
tensorboard --logdir logs/ --port 6006
# åœ¨æµè§ˆå™¨ä¸­å¯ä»¥æŒ‰æ•°æ®é›†ç­›é€‰å¯¹æ¯”
```

## ğŸ” å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©æ‰°åŠ¨çº§åˆ«ï¼Ÿ

**ç­”**: æ ¹æ®åˆå§‹æ ‡å®šè¯¯å·®é€‰æ‹©
- **å°æ‰°åŠ¨ (5Â°, 0.3m)**: æ ‡å®šè¯¯å·® < 5Â°
- **ä¸­ç­‰æ‰°åŠ¨ (10Â°, 0.5m)**: æ ‡å®šè¯¯å·® 5-10Â°ï¼ˆæ¨èï¼‰
- **å¤§æ‰°åŠ¨ (20Â°, 1.5m)**: æ ‡å®šè¯¯å·® > 10Â°

### Q2: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**ç­”**: å–å†³äºæ•°æ®é›†å¤§å°å’Œ GPU
- B26A (3178 å¸§): ~2-4 å°æ—¶ / 100 epoch (å• GPU)
- all_training_data: éœ€è¦æŸ¥çœ‹å…·ä½“å¸§æ•°

### Q3: å¦‚ä½•æ¢å¤ä¸­æ–­çš„è®­ç»ƒï¼Ÿ

**ç­”**: ä½¿ç”¨ resume æ¨¡å¼
```bash
bash train_universal.sh resume \
  --dataset_root /path/to/data \
  --dataset_name all_training_data \
  --cuda_device 0
```

### Q4: å¦‚ä½•ä¿®æ”¹æ‰¹æ¬¡å¤§å°ï¼Ÿ

**ç­”**: ç›®å‰éœ€è¦ä¿®æ”¹è„šæœ¬ä¸­çš„ `--batch_size` å‚æ•°
```bash
# ç¼–è¾‘ train_universal.sh
# å°† --batch_size 16 æ”¹ä¸ºå…¶ä»–å€¼
```

### Q5: æ—¥å¿—ç›®å½•å ç”¨ç©ºé—´å¤ªå¤§æ€ä¹ˆåŠï¼Ÿ

**ç­”**: å®šæœŸæ¸…ç†å’Œå½’æ¡£
```bash
# å½’æ¡£æ—§æ—¥å¿—
tar -czf logs_archive_$(date +%Y%m%d).tar.gz logs/B26A/

# åˆ é™¤åŸå§‹æ—¥å¿—
rm -rf logs/B26A/
```

## ğŸ“ è®­ç»ƒæ£€æŸ¥æ¸…å•

### è®­ç»ƒå‰

- [ ] æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œæ ¼å¼æ­£ç¡®
- [ ] Conda ç¯å¢ƒå·²æ¿€æ´» (`bevcalib`)
- [ ] GPU å¯ç”¨ä¸”æ˜¾å­˜å……è¶³ (`nvidia-smi`)
- [ ] bev_pool CUDA æ‰©å±•å·²ç¼–è¯‘
- [ ] ç¡®å®šè®­ç»ƒé…ç½®ï¼ˆæ•°æ®é›†ã€æ‰°åŠ¨çº§åˆ«ã€ç‰ˆæœ¬å·ï¼‰

### è®­ç»ƒä¸­

- [ ] è®­ç»ƒè¿›ç¨‹æ­£å¸¸è¿è¡Œ (`ps aux | grep train_kitti`)
- [ ] GPU åˆ©ç”¨ç‡æ­£å¸¸ (`nvidia-smi`)
- [ ] æ—¥å¿—æ­£å¸¸å†™å…¥ (`tail -f logs/.../train.log`)
- [ ] Loss æ­£å¸¸ä¸‹é™ï¼ˆTensorBoardï¼‰
- [ ] æ— å¼‚å¸¸é”™è¯¯ä¿¡æ¯

### è®­ç»ƒå

- [ ] æ£€æŸ¥ç‚¹æ–‡ä»¶å·²ä¿å­˜ï¼ˆ`*.pth`ï¼‰
- [ ] TensorBoard æ›²çº¿æ­£å¸¸
- [ ] è¯„ä¼°æ¨¡å‹æ€§èƒ½
- [ ] å¤‡ä»½é‡è¦æ¨¡å‹

## ğŸ¯ å¿«é€Ÿå‘½ä»¤å¤‡å¿˜

```bash
# ============ å¯åŠ¨è®­ç»ƒ ============
# B26A æ•°æ®é›†
bash start_training.sh B26A v1

# all_training_data æ•°æ®é›†
bash start_training.sh all v1

# ============ ç›‘æ§è®­ç»ƒ ============
# GPU çŠ¶æ€
nvidia-smi -l 1

# è®­ç»ƒè¿›ç¨‹
ps aux | grep train_kitti

# å®æ—¶æ—¥å¿—
tail -f logs/all_training_data/model_small_10deg_v1/train.log

# TensorBoard
tensorboard --logdir logs/all_training_data/ --port 6006

# ============ åœæ­¢è®­ç»ƒ ============
bash stop_training.sh
# æˆ–
pkill -f train_kitti

# ============ æ£€æŸ¥ç‚¹ç®¡ç† ============
# æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹
find logs/all_training_data/model_small_10deg_v1/ -name "*.pth" | sort -V | tail -1

# åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹
ls -lh logs/all_training_data/model_small_10deg_v1/*.pth
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `TRAINING_SCRIPT_MIGRATION.md` - è„šæœ¬é‡æ„è¯¦ç»†è¯´æ˜
- `train_universal.sh` - é€šç”¨è®­ç»ƒè„šæœ¬
- `start_training.sh` - å¯åŠ¨è„šæœ¬
- `logs/README.md` - æ—¥å¿—ç›®å½•è¯´æ˜

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä»å°æ•°æ®é›†å¼€å§‹**: å…ˆç”¨ B26A éªŒè¯é…ç½®æ­£ç¡®
2. **ç‰ˆæœ¬ç®¡ç†**: ä½¿ç”¨æœ‰æ„ä¹‰çš„ç‰ˆæœ¬å·ï¼ˆå¦‚ `v1_baseline`, `v2_tuned`ï¼‰
3. **å®šæœŸæ£€æŸ¥**: æ¯ 50-100 epoch æŸ¥çœ‹ä¸€æ¬¡ TensorBoard
4. **ä¿å­˜é‡è¦æ¨¡å‹**: åŠæ—¶å¤‡ä»½è¡¨ç°å¥½çš„æ£€æŸ¥ç‚¹
5. **æ—¥å¿—å½’æ¡£**: å®šæœŸæ¸…ç†å’Œå½’æ¡£æ—§æ—¥å¿—

---

**åˆ›å»ºæ—¥æœŸ**: 2026-03-01  
**é€‚ç”¨ç‰ˆæœ¬**: train_universal.sh + start_training.sh  
**æ¨èå·¥ä½œæµ**: start_training.sh â†’ ç›‘æ§ â†’ TensorBoard
